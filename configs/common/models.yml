# configs/common/models.yml

families:

  ###########################################################################
  # SPAcY – modèles TextCat
  ###########################################################################
  spacy:

    spacy_cnn_quick:
      desc: "TextCat spaCy CNN rapide (profil quick)"
      arch: "cnn"
      lang: "fr"
      epochs: 5
      dropout: 0.2
      eval_frequency: 500
      batch_start: 16
      batch_stop: 64
      # Template cfg à créer/adapter si besoin
      config_template: "configs/spacy/textcat_cnn_base.cfg"

    spacy_cnn_full:
      desc: "TextCat spaCy CNN complet (profil full)"
      arch: "cnn"
      lang: "fr"
      epochs: 15
      dropout: 0.2
      eval_frequency: 1000
      batch_start: 32
      batch_stop: 128
      config_template: "configs/spacy/textcat_cnn_base.cfg"

    spacy_bow_quick:
      desc: "spaCy TextCat BOW (rapide)"
      arch: "bow"
      config_template: "configs/spacy/textcat_bow_base.cfg"
      epochs: 5
      dropout: 0.1

    spacy_cnn_quick:
      desc: "spaCy TextCat CNN/Ensemble"
      arch: "cnn"
      config_template: "configs/spacy/textcat_cnn_base.cfg"
      epochs: 5
      dropout: 0.1

    # Optionnel : un modèle plus léger / debug
    spacy_cnn_debug:
      desc: "TextCat spaCy CNN très léger (debug/smoke test)"
      arch: "cnn"
      lang: "fr"
      epochs: 2
      dropout: 0.3
      eval_frequency: 200
      batch_start: 8
      batch_stop: 32
      config_template: "configs/spacy/textcat_cnn_base.cfg"


  ###########################################################################
  # SKLEARN – modèles linéaires / arbres / forêts
  ###########################################################################
  sklearn:

    # Baseline déjà existante : TF-IDF + LinearSVC
    tfidf_svm_quick:
      desc: "TFIDF + LinearSVC, avec class weights issus du balance_strategy"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 50000
          min_df: 5
      estimator:
        class: "sklearn.svm.LinearSVC"
        params:
          C: 1.0
          class_weight: "from_balance"
          # n_jobs sera fixé à hardware.max_procs si None ou -1
          # LinearSVC n'a pas n_jobs, donc on laisse vide ici.

    # SMO linéaire (toujours un SVM linéaire, mais on marque clairement SMO)
    tfidf_smo_linear:
      desc: "TFIDF + LinearSVC (SMO linéaire, baseline sérieuse)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 100000
          min_df: 3
      estimator:
        class: "sklearn.svm.LinearSVC"
        params:
          C: 0.5
          class_weight: "balanced"

    # SMO noyau RBF (SVC)
    tfidf_smo_rbf:
      desc: "TFIDF + SVC (SMO RBF, plus lourd mais flexible)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 50000
          min_df: 5
      estimator:
        class: "sklearn.svm.SVC"
        params:
          C: 1.0
          kernel: "rbf"
          gamma: "scale"
          class_weight: "balanced"
          # n_jobs n'existe pas sur SVC, donc rien ici

    # Perceptron linéaire
    tfidf_perceptron:
      desc: "TFIDF + Perceptron (classifieur linéaire en ligne)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 80000
          min_df: 3
      estimator:
        class: "sklearn.linear_model.Perceptron"
        params:
          max_iter: 50
          tol: 1e-3
          class_weight: "balanced"
          n_jobs: -1   # sera remplacé par hardware.max_procs

    # "RandomTree" → DecisionTreeClassifier
    tfidf_randomtree:
      desc: "TFIDF + DecisionTree (arbre unique, explicable)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 50000
          min_df: 5
      estimator:
        class: "sklearn.tree.DecisionTreeClassifier"
        params:
          max_depth: 50
          min_samples_split: 10
          min_samples_leaf: 5
          class_weight: "balanced"
          random_state: 42

    # RandomForest
    tfidf_randomforest:
      desc: "TFIDF + RandomForest (ensemble d'arbres)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 50000
          min_df: 5
      estimator:
        class: "sklearn.ensemble.RandomForestClassifier"
        params:
          n_estimators: 200
          max_depth: 50
          min_samples_split: 10
          min_samples_leaf: 5
          class_weight: "balanced"
          n_jobs: -1        # sera remplacé par hardware.max_procs
          random_state: 42

    tfidf_randomforest_quick:
      desc: "TFIDF + RandomForest (quick, CPU friendly)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 30000
          min_df: 5
      estimator:
        class: "sklearn.ensemble.RandomForestClassifier"
        params:
          n_estimators: 120
          max_depth: 40
          min_samples_split: 5
          min_samples_leaf: 2
          class_weight: "balanced"
          n_jobs: -1
          random_state: 42

    tfidf_decisiontree_quick:
      desc: "TFIDF + DecisionTree (quick/debug)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 20000
          min_df: 3
      estimator:
        class: "sklearn.tree.DecisionTreeClassifier"
        params:
          max_depth: 40
          min_samples_split: 5
          min_samples_leaf: 2
          class_weight: "balanced"
          random_state: 42

    tfidf_smo_quick:
      desc: "TFIDF + LinearSVC (SMO rapide)"
      vectorizer:
        class: "sklearn.feature_extraction.text.TfidfVectorizer"
        params:
          ngram_range: [1, 2]
          max_features: 60000
          min_df: 3
      estimator:
        class: "sklearn.svm.LinearSVC"
        params:
          C: 0.8
          class_weight: "balanced"


  ###########################################################################
  # HF – Transformers (CamemBERT, FlauBERT, BERT)
  ###########################################################################
  hf:

    camembert_base:
      desc: "CamemBERT base"
      model_name: "camembert-base"
      tokenizer_class: "transformers.AutoTokenizer"
      model_class: "transformers.AutoModelForSequenceClassification"
      use_class_weights: false
      trainer_params:
        learning_rate: 2e-5
        per_device_train_batch_size: 8
        per_device_eval_batch_size: 8
        num_train_epochs: 3
        weight_decay: 0.01
        warmup_ratio: 0.0
        gradient_accumulation_steps: 1

    flaubert_base_cased:
      desc: "FlauBERT-base-cased classification (FR, CPU)"
      model_name: "flaubert/flaubert_base_cased"
      tokenizer_class: "transformers.AutoTokenizer"
      model_class: "transformers.AutoModelForSequenceClassification"
      trainer_params:
        learning_rate: 2e-5
        per_device_train_batch_size: 8
        per_device_eval_batch_size: 8
        num_train_epochs: 3
        weight_decay: 0.01
        warmup_ratio: 0.1
        gradient_accumulation_steps: 1

    flaubert_quick:
      desc: "FlauBERT base (quick run)"
      model_name: "flaubert/flaubert_base_cased"
      tokenizer_class: "transformers.AutoTokenizer"
      model_class: "transformers.AutoModelForSequenceClassification"
      use_class_weights: true
      trainer_params:
        learning_rate: 2e-5
        per_device_train_batch_size: 8
        per_device_eval_batch_size: 8
        num_train_epochs: 2
        weight_decay: 0.0
        warmup_ratio: 0.0
        gradient_accumulation_steps: 1

    bert_mbert_base:
      desc: "BERT multilingue (bert-base-multilingual-cased) classification (CPU)"
      model_name: "bert-base-multilingual-cased"
      tokenizer_class: "transformers.AutoTokenizer"
      model_class: "transformers.AutoModelForSequenceClassification"
      trainer_params:
        learning_rate: 2e-5
        per_device_train_batch_size: 8
        per_device_eval_batch_size: 8
        num_train_epochs: 3
        weight_decay: 0.01
        warmup_ratio: 0.1
        gradient_accumulation_steps: 1

    bert_mbert_quick:
      desc: "BERT multilingue (quick)"
      model_name: "bert-base-multilingual-cased"
      tokenizer_class: "transformers.AutoTokenizer"
      model_class: "transformers.AutoModelForSequenceClassification"
      use_class_weights: true
      trainer_params:
        learning_rate: 3e-5
        per_device_train_batch_size: 8
        per_device_eval_batch_size: 8
        num_train_epochs: 2
        weight_decay: 0.0
        warmup_ratio: 0.0
        gradient_accumulation_steps: 1
